{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01.02 - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/ai4eng.v1/main/content/init.py\n",
    "import init, inspect; init.init(force_download=False); init.get_weblink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.lib.rlxmoocapi import submit, session\n",
    "student = session.Session(init.endpoint).login( course_id=init.course_id, \n",
    "                                                lab_id=\"L01.02\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General remark\n",
    "\n",
    "You do not need to use Python to solve the problems in this notebook, you can use any tool of your choice (Excel, etc.), including **pen and paper**. But\n",
    "\n",
    "### If you want to try out in Python\n",
    "\n",
    "- `numpy` is the Python library used for vectors\n",
    "- there are operations that take a vector and produce another vector (i.e. `np.log`)\n",
    "- there are operations that take a vector and procude a number (i.e. `np.mean`)\n",
    "- there are operations that take two vectors and produce a number (see the **HINTs** below)\n",
    "- etc.\n",
    "\n",
    "For instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = np.array([1,2,3,4])\n",
    "\n",
    "# the log of each element of the vector\n",
    "print ( \"the log   =\", np.log(v1)  )\n",
    "\n",
    "# the mean of all elements of the vector\n",
    "print ( \"the mean  =\",  np.mean(v1)  )\n",
    "\n",
    "# multiply all elements of a vector with a scalar\n",
    "print (\"times two =\", 2*v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can always check the type of any variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "type(v1), type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01. Accuracy\n",
    "\n",
    "Compute the percentage of correct predictions **accuracy** (see [here](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Definitions)) for the following model output (`predicted`) and ground truth (`actual`).\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "**CHALLENGE**: use Python with [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t1_actual    = np.random.randint(2, size=20)\n",
    "t1_predicted = np.abs(t1_actual*(np.random.random(size=20)>(np.random.random()*.9+.05)).astype(int))\n",
    "print (\"actual   \", \", \".join([str(i) for i in t1_actual]))\n",
    "print (\"predicted\", \", \".join([str(i) for i in t1_predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `accuracy` variable, **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submit your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(globals(), task_id=\"task_01\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Sensitivity\n",
    "\n",
    "Compute the sensitivity metric [aka the _True Positive Rate_ or _Recall_ see [Sensitivity on Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)] for the following model output (`predicted`) and ground truth (`actual`)\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "**Challenge**: Use Python [`sklearn.metrics.recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t2_predicted = np.random.randint(2, size=20)\n",
    "t2_actual = np.random.randint(2, size=20)\n",
    "t2_predicted[np.argwhere(t2_actual==1)[0][0]]=0\n",
    "print (\"actual   \", \", \".join([str(i) for i in t2_actual]))\n",
    "print (\"predicted\", \", \".join([str(i) for i in t2_predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `tpr` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submit your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student.submit_task(globals(), task_id=\"task_02\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluation in New York City Taxi Trip Duration Kaggle Competition\n",
    "\n",
    "Understand the data and the evaluation metric (**Root Mean Squared Logarithmic Error**, RMSLE) of the following Kaggle competition\n",
    "\n",
    "- [https://www.kaggle.com/c/nyc-taxi-trip-duration/](https://www.kaggle.com/c/nyc-taxi-trip-duration/)\n",
    "\n",
    "Observe that this competition is a **regression task** as we are measuring the difference in prediction with respect to the actual.\n",
    "\n",
    "For instance, the following model predictions and ground truth:\n",
    "\n",
    "    actual    [66 37 22]\n",
    "    predicted [79 51 67]\n",
    "    \n",
    "produce a **RMSLE** of 0.66 aprox.\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "**Challenge**: For python use numpy function `np.log` or `np.log1p`\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_actual    = np.random.randint(80,size=15)+20\n",
    "t3_predicted = np.random.randint(80,size=15)+20\n",
    "print (\"actual   \", t3_actual)\n",
    "print (\"predicted\", t3_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `rmsle` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submit your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.submit_task(globals(), task_id=\"task_03\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation in Shelter Animal Outcomes Kaggle Competition\n",
    "\n",
    "Understand the data and the evaluation metric (**Multiclass Logaritmic Loss**, _logloss_) of the following Kaggle competition\n",
    "\n",
    "- [https://www.kaggle.com/c/shelter-animal-outcomes/](https://www.kaggle.com/c/shelter-animal-outcomes/)\n",
    "\n",
    "Observe that this competition is a **classification task with 5 classes** and, for each item, the model produces a probability for each class. Classes are numbered from 0 to 4.\n",
    "\n",
    "For instance, the following represents the model output for **three items**\n",
    "\n",
    "    [[0.17 0.27 0.03 0.31 0.21]\n",
    "     [0.09 0.44 0.02 0.15 0.3 ]\n",
    "     [0.26 0.18 0.25 0.2  0.11]]\n",
    "     \n",
    "Where the classes with gretest probability assigned by the model are \n",
    "\n",
    "- class 3 for the first item (with 0.31 probability) \n",
    "- class 1 for the second item (with 0.44 probability)\n",
    "- class 0 for the third item (with 0.26 probability)\n",
    "\n",
    "The class labels are expressed as a similar matrix, but with 0/1\n",
    "For instance, the ground truth for the corresponding three items above, could be:\n",
    "\n",
    "    [[0 0 0 1 0]\n",
    "     [0 0 1 0 0]\n",
    "     [1 0 0 0 0]]\n",
    "\n",
    "and will produce a **logloss** of approx 2.14\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t4_predicted = np.random.random(size=(7,5)).T+0.5\n",
    "t4_predicted = np.round((t4_predicted/np.sum(t4_predicted,axis=0)),2).T\n",
    "\n",
    "t4_actual = np.eye(5)[np.random.randint(5,size=len(t4_predicted))].astype(int)\n",
    "\n",
    "print (\"actual\")\n",
    "print (t4_actual)\n",
    "print (\"\\npredicted\")\n",
    "print (t4_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `logloss` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submit your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.submit_task(globals(), task_id=\"task_04\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
